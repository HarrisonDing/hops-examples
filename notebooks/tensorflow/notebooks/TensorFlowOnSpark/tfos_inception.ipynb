{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A binary to train Inception in a distributed manner using multiple systems.\n",
    "Please see accompanying README.md for details and instructions.\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from tensorflowonspark import TFCluster, TFNode\n",
    "from datetime import datetime\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def main_fun(argv, ctx):\n",
    "\n",
    "  # extract node metadata from ctx\n",
    "  worker_num = ctx.worker_num\n",
    "  job_name = ctx.job_name\n",
    "  task_index = ctx.task_index\n",
    "\n",
    "  assert job_name in ['ps', 'worker'], 'job_name must be ps or worker'\n",
    "\n",
    "  from inception import inception_distributed_train\n",
    "  from inception.imagenet_data import ImagenetData\n",
    "  import tensorflow as tf\n",
    "\n",
    "  # instantiate FLAGS on workers using argv from driver and add job_name and task_id\n",
    "  print(\"argv:\", argv)\n",
    "  sys.argv = argv\n",
    "\n",
    "  FLAGS = tf.app.flags.FLAGS\n",
    "  FLAGS.job_name = job_name\n",
    "  FLAGS.task_id = task_index\n",
    "  print(\"FLAGS:\", FLAGS.__dict__['__flags'])\n",
    "\n",
    "  # Get TF cluster and server instances\n",
    "  cluster_spec, server = TFNode.start_cluster_server(ctx, 4, FLAGS.rdma)\n",
    "\n",
    "  if FLAGS.job_name == 'ps':\n",
    "    # `ps` jobs wait for incoming connections from the workers.\n",
    "    server.join()\n",
    "  else:\n",
    "    # `worker` jobs will actually do the work.\n",
    "    dataset = ImagenetData(subset=FLAGS.subset)\n",
    "    assert dataset.data_files()\n",
    "    # Only the chief checks for or creates train_dir.\n",
    "    if FLAGS.task_id == 0:\n",
    "      if not tf.gfile.Exists(FLAGS.train_dir):\n",
    "        tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "    inception_distributed_train.train(server.target, dataset, cluster_spec, ctx)\n",
    "\n",
    "\n",
    "# parse arguments needed by the Spark driver\n",
    "import argparse\n",
    "import hops.hdfs as hdfs\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--epochs\", help=\"number of epochs\", type=int, default=5)\n",
    "parser.add_argument(\"--steps\", help=\"number of steps\", type=int, default=500000)\n",
    "parser.add_argument(\"--input_data\", help=\"HDFS path to input dataset\", default=hdfs.project_path() + \"/TestJob/data/inception\")\n",
    "parser.add_argument(\"--input_mode\", help=\"method to ingest data: (spark|tf)\", choices=[\"spark\",\"tf\"], default=\"tf\")\n",
    "parser.add_argument(\"--tensorboard\", help=\"launch tensorboard process\", action=\"store_true\")\n",
    "\n",
    "(args,rem) = parser.parse_known_args()\n",
    "input_mode = TFCluster.InputMode.SPARK if args.input_mode == 'spark' else TFCluster.InputMode.TENSORFLOW\n",
    "\n",
    "print(\"{0} ===== Start\".format(datetime.now().isoformat()))\n",
    "sc = spark.sparkContext\n",
    "num_executors = int(sc._conf.get(\"spark.executor.instances\"))\n",
    "num_ps = int(sc._conf.get(\"spark.tensorflow.num.ps\"))\n",
    "\n",
    "cluster = TFCluster.run(sc, main_fun, sys.argv, num_executors, num_ps, args.tensorboard, input_mode)\n",
    "if input_mode == TFCluster.InputMode.SPARK:\n",
    "  dataRDD = sc.newAPIHadoopFile(args.input_data, \"org.tensorflow.hadoop.io.TFRecordFileInputFormat\",\n",
    "                                keyClass=\"org.apache.hadoop.io.BytesWritable\",\n",
    "                                valueClass=\"org.apache.hadoop.io.NullWritable\")\n",
    "  cluster.train(dataRDD, args.epochs)\n",
    "cluster.shutdown()\n",
    "print(\"{0} ===== Stop\".format(datetime.now().isoformat()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
