{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-GPU Training Example\n",
    "\n",
    "Train a convolutional neural network on multiple GPU with TensorFlow.\n",
    "\n",
    "This example is using TensorFlow layers, see 'convolutional_network_raw' example\n",
    "for a raw TensorFlow implementation with variables.\n",
    "\n",
    "\n",
    "- Original source: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training with multiple GPU cards\n",
    "\n",
    "In this example, we are using data parallelism to split the training accross multiple GPUs. Each GPU has a full replica of the neural network model, and the weights (i.e. variables) are updated synchronously by waiting that each GPU process its batch of data.\n",
    "\n",
    "First, each GPU process a distinct batch of data and compute the corresponding gradients, then, all gradients are accumulated in the CPU and averaged. The model weights are finally updated with the gradients averaged, and the new model weights are sent back to each GPU, to repeat the training process.\n",
    "\n",
    "<img src=\"https://www.tensorflow.org/images/Parallelism.png\" alt=\"Parallelism\" style=\"width: 400px;\"/>\n",
    "\n",
    "## CIFAR-10 Dataset Overview\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a convolutional neural network\n",
    "def conv_net(tf, x, n_classes, dropout, reuse, is_training):\n",
    "    # Define a scope for reusing the variables\n",
    "    with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "        # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "        # Reshape to match picture format [Height x Width x Channel]\n",
    "        # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "        x = tf.reshape(x, shape=[-1, 32, 32, 3])\n",
    "\n",
    "        \n",
    "        # Convolution Layer with 64 filters and a kernel size of 5\n",
    "        x = tf.layers.conv2d(x, 64, 5, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        x = tf.layers.max_pooling2d(x, 2, 2)\n",
    "\n",
    "        # Convolution Layer with 256 filters and a kernel size of 5\n",
    "        x = tf.layers.conv2d(x, 256, 3, activation=tf.nn.relu)\n",
    "        # Convolution Layer with 512 filters and a kernel size of 5\n",
    "        x = tf.layers.conv2d(x, 512, 3, activation=tf.nn.relu)\n",
    "        # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "        x = tf.layers.max_pooling2d(x, 2, 2)\n",
    "\n",
    "        # Flatten the data to a 1-D vector for the fully connected layer\n",
    "        x = tf.layers.flatten(x)\n",
    "\n",
    "        # Fully connected layer (in contrib folder for now)\n",
    "        x = tf.layers.dense(x, 2048)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        x = tf.layers.dropout(x, rate=dropout, training=is_training)\n",
    "\n",
    "        # Fully connected layer (in contrib folder for now)\n",
    "        x = tf.layers.dense(x, 1024)\n",
    "        # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "        x = tf.layers.dropout(x, rate=dropout, training=is_training)\n",
    "\n",
    "        # Output layer, class prediction\n",
    "        out = tf.layers.dense(x, n_classes)\n",
    "        # Because 'softmax_cross_entropy_with_logits' loss already apply\n",
    "        # softmax, we only apply softmax to testing network\n",
    "        out = tf.nn.softmax(out) if not is_training else out\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_input_fn(tf, filenames, batch_size=128, shuffle=False, repeat=None, num_classes=10, num_gpus=0,\n",
    "                 is_training=True):\n",
    "\n",
    "    def parser(serialized_example):\n",
    "        \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "        features = tf.parse_single_example(\n",
    "            serialized_example,\n",
    "            features={\n",
    "                'image': tf.FixedLenFeature([], tf.string),\n",
    "                'label': tf.FixedLenFeature([], tf.int64),\n",
    "            })\n",
    "        image = tf.decode_raw(features['image'], tf.uint8)\n",
    "        image = tf.cast(\n",
    "                    tf.transpose(tf.reshape(image, [3, 32, 32]), [1, 2, 0]),\n",
    "                    tf.float32)\n",
    "\n",
    "        # Normalize the values of the image from the range [0, 255] to [-0.5, 0.5]\n",
    "        image = tf.cast(image, tf.float32) / 255 - 0.5\n",
    "        label = tf.cast(features['label'], tf.int32)\n",
    "        label = tf.one_hot(label, num_classes)\n",
    "        return image, label\n",
    "\n",
    "    def _input_fn():\n",
    "        # Import MNIST data\n",
    "        dataset = tf.data.TFRecordDataset(filenames)\n",
    "\n",
    "        # Map the parser over dataset, and batch results by up to batch_size\n",
    "        dataset = dataset.map(parser).prefetch(batch_size * num_gpus * 5)\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=batch_size * num_gpus)\n",
    "        if is_training:\n",
    "            dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(batch_size * num_gpus))\n",
    "        dataset = dataset.repeat(repeat)\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "        return iterator\n",
    "\n",
    "    return _input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the function to average the gradients\n",
    "def average_gradients(tf, tower_grads):\n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        grads = []\n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "\n",
    "        # Average over the 'tower' dimension.\n",
    "        grad = tf.concat(grads, 0)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "\n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrapper():\n",
    "    from hops import devices, tensorboard, hdfs\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    import time\n",
    "    import os\n",
    "    \n",
    "    # Parameters\n",
    "    num_gpus = devices.get_num_gpus()\n",
    "    num_steps = 2000000\n",
    "    learning_rate = 0.001\n",
    "    batch_size = 2056\n",
    "    display_step = 10\n",
    "    \n",
    "    # Network Parameters\n",
    "    num_input = 32*32*3 # Cifar data input (img shape: 32*32)\n",
    "    num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "    dropout = 0.75 # Dropout, probability to keep units\n",
    "    \n",
    "    logdir = tensorboard.logdir()\n",
    "    data_dir = hdfs.project_path() + \"TestJob/data/cifar10/\"\n",
    "    train_filenames = [data_dir + \"/train/train.tfrecords\"]\n",
    "    validation_filenames = [data_dir + \"/validation/validation.tfrecords\"]\n",
    "    test_filenames = [data_dir + \"eval/eval.tfrecords\"]\n",
    "    \n",
    "    train_iterator = data_input_fn(tf, train_filenames, \n",
    "                                   batch_size=batch_size,\n",
    "                                   num_gpus=num_gpus)\n",
    "    eval_iterator = data_input_fn(tf, validation_filenames, \n",
    "                                  batch_size=batch_size, \n",
    "                                  num_gpus=num_gpus)\n",
    "    test_iterator = data_input_fn(tf, test_filenames, \n",
    "                                  is_training=False, # Read all at once\n",
    "                                  shuffle=False, \n",
    "                                  repeat=1, \n",
    "                                  num_gpus=num_gpus)\n",
    "    \n",
    "    # Place all ops on CPU by default\n",
    "    with tf.device('/cpu:0'):\n",
    "        tower_grads = []\n",
    "        reuse_vars = False\n",
    "        # Only rank 0 needs to create the directories\n",
    "        if not os.path.exists(logdir + '/train'):\n",
    "            os.mkdir(logdir + '/train')\n",
    "        if not os.path.exists(logdir + '/test'):\n",
    "            os.mkdir(logdir + '/test')\n",
    "        \n",
    "        with tf.name_scope('input'):\n",
    "            # tf Graph input\n",
    "            X = tf.placeholder(tf.float32, [None, 32, 32, 3], name=\"images\")\n",
    "            Y = tf.placeholder(tf.float32, [None, num_classes], name=\"labels\")\n",
    "\n",
    "        # Loop over all GPUs and construct their own computation graph\n",
    "        for i in range(num_gpus):\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "\n",
    "                # Split data between GPUs\n",
    "                _x = X[i * batch_size: (i+1) * batch_size]\n",
    "                _y = Y[i * batch_size: (i+1) * batch_size]\n",
    "\n",
    "                # Because Dropout have different behavior at training and prediction time, we\n",
    "                # need to create 2 distinct computation graphs that share the same weights.\n",
    "\n",
    "                # Create a graph for training\n",
    "                logits_train = conv_net(tf, _x, num_classes, dropout,\n",
    "                                        reuse=reuse_vars, is_training=True)\n",
    "                # Create another graph for testing that reuse the same weights\n",
    "                logits_test = conv_net(tf, _x, num_classes, dropout,\n",
    "                                       reuse=True, is_training=False)\n",
    "\n",
    "                with tf.name_scope('Loss'):\n",
    "                    # Define loss and optimizer (with train logits, for dropout to take effect)\n",
    "                    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                        logits=logits_train, labels=_y))\n",
    "                    \n",
    "                \n",
    "                optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "                grads = optimizer.compute_gradients(loss_op)\n",
    "\n",
    "                # Only first GPU compute accuracy\n",
    "                if i == 0:\n",
    "                    with tf.name_scope('Accuracy'):\n",
    "                        # Evaluate model (with test logits, for dropout to be disabled)\n",
    "                        correct_pred = tf.equal(tf.argmax(logits_test, 1), tf.argmax(_y, 1))\n",
    "                        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "                        \n",
    "                reuse_vars = True\n",
    "                tower_grads.append(grads)\n",
    "                \n",
    "            with tf.device('/cpu:0'):\n",
    "                if i == 0:\n",
    "                    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "                    tf.summary.scalar('loss', loss_op)\n",
    "                    tf.summary.image(\"images\", _x)\n",
    "\n",
    "        tower_grads = average_gradients(tf, tower_grads)\n",
    "        train_op = optimizer.apply_gradients(tower_grads)\n",
    "\n",
    "        # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "        merged = tf.summary.merge_all()\n",
    "        \n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "    \n",
    "        # Launch the graph\n",
    "        with tf.Session() as sess:\n",
    "            print('Initialziing')\n",
    "            sess.run(init)\n",
    "            \n",
    "            train_writer = tf.summary.FileWriter(logdir + '/train', sess.graph)\n",
    "            test_writer = tf.summary.FileWriter(logdir+ '/test', sess.graph)\n",
    "            \n",
    "            step = 1\n",
    "            # Keep training until reach max iterations\n",
    "            for step in range(1, num_steps + 1):\n",
    "                # Get a batch for each GPU\n",
    "                batch_x, batch_y = sess.run(train_iterator.get_next())\n",
    "                \n",
    "                # Run optimization op (backprop)\n",
    "                ts = time.time()\n",
    "                sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "                te = time.time() - ts\n",
    "                if (step % display_step == 0 or step == 1):\n",
    "                    # Calculate batch loss and accuracy\n",
    "                    batch_x_eval, batch_y_eval = sess.run(eval_iterator.get_next())\n",
    "\n",
    "                    loss, acc, summary = sess.run([loss_op, accuracy, merged], \n",
    "                                         feed_dict={X: batch_x_eval,\n",
    "                                                    Y: batch_y_eval})\n",
    "                    with tf.device('/cpu:0'):\n",
    "                        tf.summary.scalar(\"images/sec\", int(len(batch_x)/te))\n",
    "                        train_writer.add_summary(summary, step)\n",
    "                step += 1\n",
    "\n",
    "            # Calculate accuracy for 1000 mnist test images\n",
    "            batch_x_test, batch_y_test = sess.run(test_iterator.get_next())\n",
    "            num_test = len(batch_x_test)\n",
    "            loss, acc, summary = sess.run([loss_op, accuracy, merged], \n",
    "                                          feed_dict={X: batch_x_test,\n",
    "                                                     Y: batch_y_test})\n",
    "            with tf.device('/cpu:0'):\n",
    "                test_writer.add_summary(summary, step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import tflauncher\n",
    "tflauncher.launch(spark, wrapper)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
