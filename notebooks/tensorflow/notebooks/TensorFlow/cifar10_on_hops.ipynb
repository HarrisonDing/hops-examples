{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started: Cifar-10 on Hops Notebook\n",
    "---\n",
    "\n",
    "In this notebook we are going to master running Deep Learning code on Hops using Jupyter notebooks and HopsFS, the highly efficient distributed file system that Hops provides. HopsFS is a fork of Apache HDFS and operations for reading and writing files to HopsFS is the same as with HDFS that TensorFlow supports.\n",
    "For more information about how to read from HDFS, please see: https://www.tensorflow.org/deploy/hadoop\n",
    "\n",
    "In this example program we are going to:\n",
    "- Run Cifar-10 code using `tf.contrib.learn.Experiment` and `tf.estimator.Estimator`\n",
    "- Using the hops python library to run TensorFlow on the Hops Platform\n",
    "- Read data from a Dataset which is located in your project, on HopsFS (HDFS)\n",
    "- Define a convolutional neural network\n",
    "- Define a set of hyperparameters and values to perform gridsearch on\n",
    "- Run training for each combination of hyperparameters using the Experiment API\n",
    "- Monitor training using TensorBoard to see accuracy and loss for every hyperparameter configuration\n",
    "- Monitoring training by looking at logs\n",
    "- Find where the model and TensorBoard events for each hyperparameter configuration is saved in your project\n",
    "- Visualize TensorBoard events from the previous TensorBoard run\n",
    "\n",
    "## Table of contents:\n",
    "\n",
    "### [TensorFlow on Hops](#paradigm)\n",
    "### [Training Cifar-10 on Hops](#cifar10)\n",
    "### [Hyperparameter search](#hyperparam)\n",
    "### [Launching TensorFlow jobs](#starting)\n",
    "### [Monitoring execution - TensorBoard](#tensorboard)\n",
    "### [Monitoring execution - Logs](#logs)\n",
    "### [Increasing throughput - Running jobs in parallel](#parallel)\n",
    "### [Show TensorBoard from previous runs](#visualize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hops TensorFlow programming paradigm <a class=\"anchor\" id='paradigm'></a>\n",
    "\n",
    "To be able to run your TensorFlow code on Hops, the code for the whole program needs to be provided and put inside a wrapper function. Everything, from importing libraries to reading data and defining the model and running the program needs to be put inside a wrapper function. If you wish to run gridsearch over a given set of hyperparameters, you can define arguments for this wrapper function that corresponds to the name of your hyperparameters.\n",
    "\n",
    "You can also submit one or more `.py`, `.zip` or `.egg` files that contain your code and import them in the wrapper function. To include files, navigate back to HopsWorks and restart restart Jupyter, you can then include files in the Jupyter configuration.\n",
    "\n",
    "## The `hops` python module\n",
    "\n",
    "Below you can see the aforementioned wrapper function, which is coincidently named `wrapper` but could potentially be named anything. You can see two imports from the `hops` module, a `tensorboard` and an `hdfs` module. These are the only two modules that you will need to use in your TensorFlow wrapper function. \n",
    "\n",
    "### Using the `tensorboard` module\n",
    "The `tensorboard` module allow us to get the log directory for summaries and checkpoints to be written to the TensorBoard we will see in a bit. The only function that we currently need to call is `tensorboard.logdir()`, which returns the path to the TensorBoard log directory. Furthermore, the content of this directory will be put in as a Dataset in your project in HopsFS after each hyperparameter configuration is finished. The `tflaunch.launch` function, that we will look at abit further down will return the exact path, which you can then navigate to using HopsWorks to inspect the files.\n",
    "\n",
    "The directory could in practice be used to store other data that should be accessible after each hyperparameter configuration is finished.\n",
    "```python\n",
    "# Use this module to get the TensorBoard logdir\n",
    "from hops import tensorboard\n",
    "tensorboard_logdir = tensorboard.logdir()\n",
    "```\n",
    "\n",
    "\n",
    "### Using the `hdfs` module\n",
    "The `hdfs` module provides a single method to get the path in HopsFS where your data is stored, namely by calling `hdfs.project_path()`. The path resolves to the root path for your project, which is the view that you see when you click `Data Sets` in HopsWorks. To point where your actual data resides in the project you to append the full path from there to your Dataset. For example if you create a cifar10 folder in your Resources Dataset, which is created automatically for each project, the path to the cifar10 data would be `hdfs.project_path() + 'Resources/cifar10`\n",
    "```python\n",
    "# Use this module to get the path to your project in HopsFS, \n",
    "# then append the path to your Dataset in your project\n",
    "from hops import hdfs\n",
    "project_path = hdfs.project_path()\n",
    "```\n",
    "\n",
    "![image11-Dataset-ProjectPath.png](../images/datasets.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Cifar10 on Hops <a class=\"anchor\" id='cifar10'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The wrapper function\n",
    "\n",
    "Here we define the aforementioned wrapper function containing the code to run, with the hyperparameter arguments `learning_rate` and `dropout`. It simply contains all the TensorFlow code that we want to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the TensorFlow code you want to run in a function\n",
    "# To perform hyper-parameter searching define your parameters as arguments\n",
    "\n",
    "def wrapper(learning_rate, dropout):\n",
    "    \n",
    "    import tensorflow as tf\n",
    "    \n",
    "    # Use this module to get the TensorBoard logdir\n",
    "    from hops import tensorboard\n",
    "    \n",
    "    # Use this module to get the path to your project in HopsFS, then append the path to your Dataset in your project\n",
    "    from hops import hdfs\n",
    "    \n",
    "    num_classes = 10 # CIFAR10 total classes (0-9 objects)\n",
    "    num_steps = 200\n",
    "    batch_size = 128\n",
    "\n",
    "    # Network Parameters\n",
    "    num_input = 32*32*3 # CIFAR10 data input (img shape: 32*32*3)\n",
    "    \n",
    "    # Create the neural network\n",
    "    # TF Estimator input is a dict, in case of multiple inputs\n",
    "    def conv_net(x, n_classes, dropout, reuse, is_training):\n",
    "        \n",
    "\n",
    "        # Define a scope for reusing the variables\n",
    "        with tf.variable_scope('ConvNet', reuse=reuse):\n",
    "\n",
    "            # MNIST data input is a 1-D vector of 784 features (28*28 pixels)\n",
    "            # Reshape to match picture format [Height x Width x Channel]\n",
    "            # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "            x = tf.reshape(x, shape=[-1, 32, 32, 3])\n",
    "            #x = tf.image.crop_and_resize(tf.reshape(x, shape=[-1, 32, 32, 3]),size=[24,24])\n",
    "\n",
    "            # Convolution Layer with 32 filters and a kernel size of 5\n",
    "            conv1 = tf.layers.conv2d(x, 64, 5, activation=tf.nn.relu)\n",
    "            # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "            conv1 = tf.layers.max_pooling2d(conv1, 2, 2)\n",
    "\n",
    "            # Convolution Layer with 32 filters and a kernel size of 5\n",
    "            conv2 = tf.layers.conv2d(conv1, 64, 5, activation=tf.nn.relu)\n",
    "            # Max Pooling (down-sampling) with strides of 2 and kernel size of 2\n",
    "            conv2 = tf.layers.max_pooling2d(conv2, 2, 2)\n",
    "\n",
    "            # Flatten the data to a 1-D vector for the fully connected layer\n",
    "            fc1 = tf.contrib.layers.flatten(conv2)\n",
    "\n",
    "            # Fully connected layer (in tf contrib folder for now)\n",
    "            fc1 = tf.layers.dense(fc1, 384)\n",
    "            # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "            fc1 = tf.layers.dropout(fc1, rate=dropout, training=is_training)\n",
    "\n",
    "            fc2 = tf.layers.dense(fc1, 192)\n",
    "            # Apply Dropout (if is_training is False, dropout is not applied)\n",
    "            fc2 = tf.layers.dropout(fc2, rate=dropout, training=is_training)\n",
    "            \n",
    "            # Output layer, class prediction\n",
    "            out = tf.layers.dense(fc2, n_classes)\n",
    "\n",
    "            return out\n",
    "    \n",
    "    # Define the model function (following TF Estimator Template)\n",
    "    def model_fn(features, labels, mode, params):\n",
    "\n",
    "        # Build the neural network\n",
    "        # Because Dropout have different behavior at training and prediction time, we\n",
    "        # need to create 2 distinct computation graphs that still share the same weights.\n",
    "        logits_train = conv_net(features, num_classes, dropout, reuse=False, is_training=True)\n",
    "        logits_test = conv_net(features, num_classes, dropout, reuse=True, is_training=False)\n",
    "\n",
    "        # Predictions\n",
    "        pred_classes = tf.argmax(logits_test, axis=1)\n",
    "        pred_probas = tf.nn.softmax(logits_test)\n",
    "\n",
    "        # If prediction mode, early return\n",
    "        if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "            return tf.estimator.EstimatorSpec(mode, predictions=pred_classes)\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        logits=logits_train, labels=tf.cast(labels, dtype=tf.int32)))\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss_op, global_step=tf.train.get_global_step())\n",
    "\n",
    "        # Evaluate the accuracy of the model\n",
    "        acc_op = tf.metrics.accuracy(labels=labels, predictions=pred_classes)\n",
    "\n",
    "        image = tf.reshape(features[:10], [-1, 32, 32, 3])\n",
    "        tf.summary.image(\"image\", image)\n",
    "\n",
    "        # TF Estimators requires to return a EstimatorSpec, that specify\n",
    "        # the different ops for training, evaluating, ...\n",
    "        estim_specs = tf.estimator.EstimatorSpec(\n",
    "          mode=mode,\n",
    "          predictions=pred_classes,\n",
    "          loss=loss_op,\n",
    "          train_op=train_op,\n",
    "          eval_metric_ops={'accuracy': acc_op})\n",
    "\n",
    "        return estim_specs\n",
    "    \n",
    "    def data_input_fn(filenames, num_input, batch_size=1000, shuffle=False, repeat=None):\n",
    "    \n",
    "        def parser(serialized_example):\n",
    "            \"\"\"Parses a single tf.Example into image and label tensors.\"\"\"\n",
    "            features = tf.parse_single_example(\n",
    "                serialized_example,\n",
    "                features={\n",
    "                    'image': tf.FixedLenFeature([], tf.string),\n",
    "                    'label': tf.FixedLenFeature([], tf.int64),\n",
    "                })\n",
    "            image = tf.decode_raw(features['image'], tf.uint8)\n",
    "            image.set_shape([num_input])\n",
    "\n",
    "            image = tf.cast(\n",
    "                tf.transpose(tf.reshape(image, [3, 32, 32]), [1, 2, 0]),\n",
    "                tf.float32)\n",
    "\n",
    "            # Normalize the values of the image from the range [0, 255] to [-0.5, 0.5]\n",
    "            image = tf.cast(image, tf.float32) / 255 - 0.5\n",
    "            label = tf.cast(features['label'], tf.int32)\n",
    "            return image, label\n",
    "\n",
    "        def _input_fn():\n",
    "            # Import CIFAR10 data\n",
    "            dataset = tf.contrib.data.TFRecordDataset(filenames)\n",
    "\n",
    "            # Map the parser over dataset, and batch results by up to batch_size\n",
    "            dataset = dataset.map(parser, num_threads=1, output_buffer_size=batch_size)\n",
    "            if shuffle:\n",
    "                dataset = dataset.shuffle(buffer_size=256)\n",
    "            dataset = dataset.batch(batch_size)\n",
    "            dataset = dataset.repeat(repeat)\n",
    "            iterator = dataset.make_one_shot_iterator()\n",
    "\n",
    "            features, labels = iterator.get_next()\n",
    "\n",
    "            return features, labels\n",
    "\n",
    "        return _input_fn\n",
    "    \n",
    "    # Path to TensorBoard logdir on executor\n",
    "    # Its contents will be placed in your project path /Logs/TensorFlow/appId/runId/ after the job is finished\n",
    "    logdir = tensorboard.logdir()\n",
    "    \n",
    "    # Path to your project in HopsFS, parent folder for your DataSets (Resources, Logs etc)\n",
    "    data_dir = hdfs.project_path()\n",
    "    train_filenames = [data_dir + \"TestJob/data/cifar10/train/train.tfrecords\"]\n",
    "    validation_filenames = [data_dir + \"TestJob/data/cifar10/validation/validation.tfrecords\"]\n",
    "\n",
    "    run_config = tf.contrib.learn.RunConfig(\n",
    "        model_dir=logdir,\n",
    "        log_device_placement=True,\n",
    "        save_checkpoints_steps=20,\n",
    "        save_summary_steps=10,\n",
    "        log_step_count_steps=10)\n",
    "    \n",
    "    hparams = tf.contrib.training.HParams(\n",
    "        learning_rate=learning_rate, dropout_rate=dropout)\n",
    "\n",
    "    summary_hook = tf.train.SummarySaverHook(\n",
    "          save_steps = run_config.save_summary_steps,\n",
    "          scaffold= tf.train.Scaffold(),\n",
    "          summary_op=tf.summary.merge_all())\n",
    "\n",
    "    mnist_estimator = tf.estimator.Estimator(\n",
    "        model_fn=model_fn,\n",
    "        config=run_config,\n",
    "        params=hparams\n",
    "    )\n",
    "\n",
    "    train_input_fn = data_input_fn(train_filenames[0], num_input, batch_size=batch_size)\n",
    "    eval_input_fn = data_input_fn(validation_filenames[0], num_input, batch_size=batch_size)\n",
    "    \n",
    "    experiment = tf.contrib.learn.Experiment(\n",
    "        mnist_estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=eval_input_fn,\n",
    "        train_steps=num_steps,\n",
    "        min_eval_frequency=10,\n",
    "        eval_hooks=[summary_hook]\n",
    "    )\n",
    "\n",
    "    experiment.train_and_evaluate()\n",
    "    \n",
    "    experiment.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter search using `grid_params` method <a class=\"anchor\" id='hyperparam'></a>\n",
    "\n",
    "Hyperparameter optimization is critical to achieve the best accuracy for your model. With Hops, hyperparameter optimization is easier than ever.  We can find the best hyperparameters to train the model and make it easy to find the best set of hyper parameters by visualizing them in TensorBoard for you.\n",
    "\n",
    "To define the hyperparameters you wish to perform gridsearch with, simply create a dictionary with the keys matching the arguments of your wrapper function, and a list of values for each parameter like below.\n",
    "\n",
    "```python\n",
    "# Arguments to use in training\n",
    "args_dict = {'learning_rate': [0.001, 0.0005, 0.0001], 'dropout': [0.45, 0.7]}\n",
    "```\n",
    "\n",
    "Next step is to generate all possible combinations, the grid, of the hyperparameters.\n",
    "```python\n",
    "# Create a grid with the specified arguments\n",
    "args_dict_grid = util.grid_params(args_dict)\n",
    "```\n",
    "\n",
    "So in effect, this is interpreted as that you want to run your TensorFlow code with 6 different hyperparameter combinations as shown in the table.\n",
    "\n",
    "\n",
    "| Job number | Learning rate | Dropout |\n",
    "|:----------:|:-------------:|:-------:|\n",
    "|      1     |     0.001     |   0.45  |\n",
    "|      2     |     0.001     |   0.7   |\n",
    "|      3     |     0.0005    |   0.45  |\n",
    "|      4     |     0.0005    |   0.7   |\n",
    "|      5     |     0.0001    |   0.45  |\n",
    "|      6     |     0.0001    |   0.7   |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from hops import util\n",
    "\n",
    "#Define dict for hyperparameters\n",
    "args_dict = {'learning_rate': [0.001, 0.0005, 0.0001], 'dropout': [0.45, 0.7]}\n",
    "\n",
    "# Generate a grid for the given hyperparameters\n",
    "args_dict_grid = util.grid_params(args_dict)\n",
    "\n",
    "print(args_dict_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the training using `tflauncher` module <a class=\"anchor\" id='starting'></a>\n",
    "\n",
    "The last, and arguably most important module that we demonstrate from the `hops` module is called `tflauncher`. The `tflauncher` module provides a single function called `launch`. As arguments it simply takes the `spark` SparkSession object, which is automatically created when the first cell is evaluated in the notebook, in addition to the wrapper function and the dictionary with the hyperparameters. `tflauncher.launch` will simply run the wrapper function and inject the value of each hyperparameter that you have specified. Assuming that you started Jupyter with the default configuration, each job will be run sequentially. To increase throughput by running two or more jobs in parallel, please see [this](#parallel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import tflauncher\n",
    "\n",
    "tensorboard_hdfs_logdir = tflauncher.launch(spark, wrapper, args_dict_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring execution - TensorBoard <a class=\"anchor\" id='tensorboard'></a>\n",
    "To find the TensorBoard for the execution, please go back to HopsWorks and follow the arrows in the images below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image7-Monitor.png](../images/jupyter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image4-LaunchTensorboard.png](../images/overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image8-Tensorboard.png](../images/tensorboard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring execution - logs <a class=\"anchor\" id='logs'></a>\n",
    "To find the logs for the execution, please go back to HopsWorks and follow the arrows in the images below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image6-GoToLogs.png](../images/logs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image9-Logs.png](../images/viewlogs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increasing throughput - Running jobs in parallel <a class=\"anchor\" id='parallel'></a>\n",
    "The default configuration is to run each job sequentially, so each one of the 6 hyperparameter configuration is run sequentially. It is possible to increase the number of jobs that can be executed at any time by increasing the value of the configuration property shown in the picture below. To do this you need to restart Jupyter, by navigating back to HopsWorks and shutting it down.\n",
    "![parallel.png](../images/parallel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing TensorBoard events from previous runs <a class=\"anchor\" id='visualize'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import tensorboard\n",
    "\n",
    "# Visualize all TensorBoard events for the jobs in the same TensorBoard\n",
    "tensorboard.visualize(spark, tensorboard_hdfs_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image12-TensorboardAgg.png](../images/visualize.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Moving on from here: Scaling out with TensorFlowOnSpark and Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We walked through how to run one of most popular deep learning architectures, Cifar-10 dataset on hops using hops `hdfs` and `tensorboard` utility modules. \n",
    "\n",
    "Next step is to reduce training time by training the model in parallel. We covered in-depth tutorials about different parallelization concepts in the other notebooks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
