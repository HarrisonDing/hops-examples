{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10 distributed training using Horovod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'> <h3>To run the code in this notebook, go to [launch_horovod.ipynb](launch_horovod.ipynb). Running this notebook will not enable Horovod and results in errors!</h3></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading up [basics about Hops Notebook and local training models on Hops](../TensorFlow/cifar10_on_hops.ipynb) it's time to scale out our training. We already discussed [multi-gpu training with TensorFlow](../TensorFlow/multigpu/Multi-gpu_training_cifar.ipynb) but the key difference here is using [Horovod](https://github.com/uber/horovod) library to parallelize the training. The reason to do this is great scaling capabilities this library has in compare to traditional methods due to using MPI and Ring allreduce concepts for parallelization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If you want a simpler dataset such as **mnist**, we have a [notebook](mnist.ipynb) for that too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tflearn.data_utils import shuffle, to_categorical\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_augmentation import ImageAugmentation\n",
    "import horovod.tensorflow as hvd\n",
    "\n",
    "from hops import tensorboard\n",
    "from hops import hdfs\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initilizaing hyper-parameters\n",
    "\n",
    "Beside usual hyper-parameters for deep neural network like number of epochs and batch size, for defining the path to our data and log directory for tensorboard, note the usage of two handy modules in `hops` library:\n",
    "1. `tensorboard.logdir()`: Get the path to your log directory so you can save checkpoints and summaries in this folder.\n",
    "2. `hdfs.project_path()`: Get the path to your main Dataset folder, after that you need to specify project name and any folder inside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Data function\n",
    "\n",
    "For processing the data using `tf.Dataset` api, we provide a utility function to build the `iterator` over the train or test dataset depending on which file names we provide as parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "num_input = 32*32*3\n",
    "batch_size = 2048\n",
    "num_classes = 10\n",
    "\n",
    "# Adjust number of epochs based on number of GPUs.\n",
    "epochs = 10\n",
    "\n",
    "last_step = 100000\n",
    "evaluation_step = 10\n",
    "\n",
    "lr = 0.003\n",
    "\n",
    "logdir = tensorboard.logdir()\n",
    "data_dir = hdfs.project_path() + \"TestJob/data/cifar10/\"\n",
    "train_filenames = [data_dir + \"train/train.tfrecords\"]\n",
    "validation_filenames = [data_dir + \"validation/validation.tfrecords\"]\n",
    "test_filenames = [data_dir + \"test/eval.tfrecords\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional network building\n",
    "def model(features):\n",
    "    net = tf.reshape(features, [-1, 32, 32, 3])\n",
    "    tf.summary.image(\"image\", net)\n",
    "    network = conv_2d(net, 32, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = conv_2d(network, 64, 3, activation='relu')\n",
    "    network = max_pool_2d(network, 2)\n",
    "    network = fully_connected(network, 512, activation='relu')\n",
    "    network = dropout(network, 0.5)\n",
    "    logits  = fully_connected(network, 10, activation='linear')\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Horovod\n",
    "\n",
    "Until now everything was quite standard in Deep learning techniques and Hops concepts. For running our code using Horovod however, we need to modify just few lines of our code. This few lines will enable us to scale our model potentially on hundreds of GPUs very well, providing optimal data input pipeline and fast GPUs connection between each other. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 Step to scale out with Horovod:\n",
    "\n",
    "#### 1. Initilization using `hvd.init()`\n",
    "#### 2. Making our optimizer Horovod-friendly:\n",
    "```python\n",
    "opt = tf.train.RMSPropOptimizer(lr)\n",
    "\n",
    "# Wrap with Distributed Optimizer\n",
    "opt = hvd.DistributedOptimizer(opt)\n",
    "```\n",
    "\n",
    "####  3. Broadcasting: \n",
    "Broadcast initial variable states from rank 0 to all other processes. This is necessary to ensure consistent initialization of all workers when training is started with random weights or restored from a checkpoint.\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "####  4. Providing `Config`: \n",
    "Pin GPU to be used to process local rank (one GPU per process)\n",
    "\n",
    "```python\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "```\n",
    "####  5. Mind the rank:\n",
    "As now we have potential tens or hundreds of GPUs running the same code, we need to take care of checkpointing and writing summaries. More specifically, we need to do this actions on only one machine. For this we need to check the rank of GPU before each of these actions:\n",
    "\n",
    "```python\n",
    "# Creating different directories for writing train and test summaries\n",
    "if hvd.local_rank()==0:\n",
    "    if not os.path.exists(logdir + '/train'):\n",
    "        os.mkdir(logdir + '/train')\n",
    "    if not os.path.exists(logdir + '/test'):\n",
    "        os.mkdir(logdir + '/test')         \n",
    "```\n",
    "\n",
    "And for actually writing them during training:\n",
    "```python\n",
    "\n",
    "if hvd.local_rank()==0:\n",
    "    train_writer = tf.summary.FileWriter(logdir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(logdir+ '/test', sess.graph)\n",
    "    \n",
    "# While in the loop\n",
    "if hvd.local_rank()==0:\n",
    "    train_writer.add_summary(summary, step)\n",
    "```\n",
    "#### 6. Run broadcasting op:\n",
    "Don't forget to run broadcasting operation which we defined earlier after initialization of variables:\n",
    "\n",
    "```python\n",
    "init.run()\n",
    "bcast.run()\n",
    "```\n",
    "#### 7. Launch it:\n",
    "\n",
    "Congrats, we finished adapting our code to use Horovod, but for actually start training, we need to have another notebook. Refer to [launch_horovod.ipynb](launch_horovod.ipynb) notebook for further instruction on how to run and monitor your training process. \n",
    "\n",
    "<font color='red'> <h4>To launch this notebook, go to [launch_horovod.ipynb](launch_horovod.ipynb). Running this notebook will not enable Horovod and results in errors!</h4></font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    # Initialize Horovod\n",
    "    hvd.init()\n",
    "\n",
    "    # Build model...\n",
    "    with tf.name_scope('input'):\n",
    "        # Placeholders for data and labels\n",
    "        X = tf.placeholder(shape=(None, 32, 32, 3), dtype=tf.float32)\n",
    "        Y = tf.placeholder(shape=(None, 10), dtype=tf.float32)\n",
    "\n",
    "    preds = model(X)\n",
    "    # Defining other ops using Tensorflow\n",
    "    with tf.name_scope('Loss'):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=Y))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    opt = tf.train.RMSPropOptimizer(lr)\n",
    "\n",
    "    # Wrap with Distributed Optimizer\n",
    "    opt = hvd.DistributedOptimizer(opt)\n",
    "\n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    train_op = opt.minimize(loss, global_step=global_step)\n",
    "\n",
    "    with tf.name_scope('Accuracy'):\n",
    "        accuracy = tf.reduce_mean(\n",
    "            tf.cast(tf.equal(tf.argmax(preds, 1), tf.argmax(Y, 1)), tf.float32),\n",
    "            name='acc')\n",
    "        # create a summary for our accuracy\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "\n",
    "\n",
    "    # Merge all the summaries and write them out to log directory\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    # Initializing the variables\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Broadcast initial variable states from rank 0 to all other processes.\n",
    "    # This is necessary to ensure consistent initialization of all workers when\n",
    "    # training is started with random weights or restored from a checkpoint.\n",
    "    bcast = hvd.broadcast_global_variables(0)\n",
    "\n",
    "    # Get train and test iterators\n",
    "    train_input_iterator = data_input_fn(train_filenames, batch_size=batch_size)\n",
    "    test_input_iterator =  data_input_fn(test_filenames)\n",
    "\n",
    "    # Pin GPU to be used to process local rank (one GPU per process)\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.visible_device_list = str(hvd.local_rank())\n",
    "\n",
    "    # Only rank 0 needs to create the directories\n",
    "    if hvd.local_rank()==0:\n",
    "        if not os.path.exists(logdir + '/train'):\n",
    "            os.mkdir(logdir + '/train')\n",
    "        if not os.path.exists(logdir + '/test'):\n",
    "            os.mkdir(logdir + '/test')\n",
    "\n",
    "    with tf.Session(config=config) as sess:\n",
    "        print('Initializing...')\n",
    "        init.run()\n",
    "        bcast.run()\n",
    "        \n",
    "        if hvd.local_rank()==0:\n",
    "            train_writer = tf.summary.FileWriter(logdir + '/train', sess.graph)\n",
    "            test_writer = tf.summary.FileWriter(logdir+ '/test', sess.graph)\n",
    "        \n",
    "        step = 0\n",
    "        while step < last_step:\n",
    "            # Run a training step synchronously.\n",
    "            images_, labels_ = sess.run(train_input_iterator.get_next())\n",
    "            _, loss_, step, summary = sess.run([train_op, loss, global_step, merged],\n",
    "                feed_dict={X: images_, Y: labels_})\n",
    "            \n",
    "            if hvd.local_rank()==0:\n",
    "                train_writer.add_summary(summary, step)\n",
    "            print('Step: {}, Loss:{}'.format(str(step), str(loss_)))\n",
    "            if step % evaluation_step == 0:\n",
    "                images_test, labels_test = sess.run(test_input_iterator.get_next())\n",
    "                acc_, loss_, step, summary = sess.run([accuracy, loss, global_step, merged],\n",
    "                    feed_dict={X: images_test, Y: labels_test})\n",
    "                if hvd.local_rank()==0:\n",
    "                    test_writer.add_summary(summary, step)\n",
    "                print(\"Step:\", '%03d' % step,\n",
    "                      \"Loss:\", str(loss_),\n",
    "                      \"Accuracy:\", str(acc_))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Don't forget:\n",
    "To call the main function if you wrapped your model logic in the `main()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting!\")\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
